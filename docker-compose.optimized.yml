version: '3.8'

services:
  flux-api:
    build:
      context: .
      dockerfile: Dockerfile.optimized
      args:
        - BUILDKIT_INLINE_CACHE=1
    container_name: flux-api-optimized
    restart: unless-stopped
    ports:
      - "8000:8000"
    
    # GPU configuration optimized for RTX 4070 Super
    deploy:
      resources:
        limits:
          memory: 16G  # Reasonable limit given your 96GB RAM
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # Environment variables for optimal performance
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      
      # PyTorch optimizations
      - TORCH_CUDNN_V8_API_ENABLED=1
      - CUDA_LAUNCH_BLOCKING=0
      - CUDA_CACHE_DISABLE=0
      - CUBLAS_WORKSPACE_CONFIG=:4096:8
      
      # Memory optimization for large models
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,garbage_collection_threshold:0.6
      
      # FastAPI/Uvicorn optimizations
      - WEB_CONCURRENCY=1  # Single worker for GPU workloads
      - MAX_WORKERS=1
      
      # App-specific settings
      - MODEL_CACHE_DIR=/app/app/model_cache
      - LOG_LEVEL=info
    
    # Volume mounts for persistence and performance
    volumes:
      # Persistent model cache (crucial for performance)
      - model_cache:/app/app/model_cache
      # Shared memory for better performance
      - /dev/shm:/dev/shm:rw
      # Optional: Mount logs directory
      - ./logs:/app/logs:rw
    
    # Network configuration
    networks:
      - flux-network
    
    # Security options
    user: "1000:1000"  # Run as non-root user
    security_opt:
      - no-new-privileges:true
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource limits and system optimizations
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    
    # Ensure GPU access
    runtime: nvidia

  # Optional: Redis for advanced queuing (future enhancement)
  redis:
    image: redis:7-alpine
    container_name: flux-redis
    restart: unless-stopped
    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - flux-network
    profiles:
      - redis  # Only start with --profile redis

  # Optional: Monitoring with Prometheus metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: flux-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - flux-network
    profiles:
      - monitoring

# Named volumes for persistence
volumes:
  model_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./app/model_cache
  redis_data:
    driver: local
  prometheus_data:
    driver: local

# Dedicated network for service communication
networks:
  flux-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16